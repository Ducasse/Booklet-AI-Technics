!! A-Priori Algorithm for Mining Frequent Itemsets

Modern retailers collect massive amounts of sales data - the logs of online and offline transactions that are commonly referred to as the ''basket data''. This data allows us to analyse customer behavior and make better management decisions. Which items should we put on sale? How to group items together on the shelves? Which items should we recommend to a specific customer?

Many of these problems can be solved by scanning the database of transactions and identifying ''frequent itemsets'' - the groups of products that are often purchased together. If we know that customers tend to buy bread, butter, and milk together, then we can place them close to each other.

By finding frequent sets of items, we can also produce association rules between products. For example, if we realise that bread, butter, and milk appear together in more than 5\% of transactions, we can go further and calculate that 90\% of customers who purchase bread and butter, also purchase milk. This gives us an association rule {bread, butter} {{{$\Rightarrow$}}} {milk} that can be used to recommend products to customers or sell them together with a discount.

In this chapter, we discuss ''A-Priori'' - a fast and efficient algorithm for mining frequent itemsets and finding association rules.

!!! What is it?
@sec:APriori-WhatIsIt

A-Priori was originally designed by Agrawal et al. ${cite:Agra94a}$ to discover association rules between items in a large database if sales transactions.

!!! Applications
@sec:APriori-Applications

The applications of A-Priori go far beyond market basket analysis.


!!! Formal Definition
@sec:APriori-Definition

Before going further, we must give a formal definition of the problem and the A-Priori algorithm. This section will contain a lot of math, but we believe that going slowly through the math and helping you understand the algorithm in all its details is vital before we start implementing it.

!!!! The Problem of Mining Frequent Itemsets

We are given a set of items called ''item base''

{{{
\[ B = \{ i_1, \dots, i_n \} \]
}}}

And a set of ''transactions'' where each transaction is a set of items from the item base {{{$B$}}}

{{{
\[ T = \{ t_1, \dots, t_m \} \]
\[ t_k \subseteq B \quad \forall t_k \in T \]
}}}

You can think of items as products that are sold at the supermarket, and each transaction as a list of products purchased by one customer (list of products on the receipt).

We use the word ''itemset'' to denote any set of items selected from the item base:

{{{
\[ I \subseteq B \]
}}}

In fact, every transaction is an itemset (a set of items that were purchased by acustomer) and the item base itself is also an itemset (a set of all products in the store).

The ''cover'' of an itemset {{{$I$}}} in a set of transactions {{{$T$}}} is a subset of transactions that contain all items from this itemset

{{{
\[ K_T(I) = \{ t_k \in T | I \subseteq t_k \} \]
}}}

The ''count'' of an item set {{{$I$}}} in a set of trasactions {{{$T$}}} is the number of transactions in which this itemset appears

{{{
\[ \eta_T(I) = | K_T(I) | \]
}}}

The ''support'' of an itemset {{{$I$}}} is the relative frequency of this itemset in a set of transactions {{{$T$}}}

{{{
\[ s_T(I) = \frac{\eta_T(I)}{|T|} \]
}}}

where {{{$|T|$}}} is the total number of transactions. Support of an itemset is the percentage of transactions that contain this all of its items.

Given some ''minimum support'' {{{$ s_{min} \in \mathbb{N} $}}}, an itemset {{{$I$}}} is called ''frequent'' if its support {{{$s_T(I)$}}} is greater than or equal to the minimum support. We can define a set of all itemsets that are frequent in a set of transactions {{{$T$}}}:

{{{
\[ F_T(s_{min}) = \{ I \subseteq B | s_T(I) \geq s_{min} \} \]
}}}

The goal of frequent itemset mining is to find the set of all frequent itemsets {{{$F_T(s_{min})$}}} given a set of transactions {{{$T$}}} and a minimum support {{{$s_{min}$}}}.

!!!! Extension of the problem: Association Rules Mining

Once we have found the set of frequent itemsets {{{$F_T(s_{min})$}}}, we can represent each itemset {{{$Q \in F_T(s_{min})$}}} as a rule in form {{{$I \Rightarrow J$}}} where itemset {{{$I$}}} is a subset of {{{$Q$}}} such that {{{$I \neq \varnothing$}}} and {{{$I \neq Q$}}} and itemset {{{$J$}}} is a complement {{{$Q \setminus I$}}}.

For example, itemset {{{$\{ bread, butter, milk \}$}}} produces 6 association rules:

{{{
\[ \{ bread, butter \} \Rightarrow \{ milk \} \]
\[ \{ bread, milk \} \Rightarrow \{ butter \} \]
\[ \{ butter, milk \} \Rightarrow \{ bread \} \]
\[ \{ bread \} \Rightarrow \{ butter, milk \} \]
\[ \{ butter \} \Rightarrow \{ bread, milk \} \]
\[ \{ milk \} \Rightarrow \{ bread, butter \} \]
}}}

In the context of basket analysis, association rules can be interpreted in the following way:

- {{{$\{ beer, peanuts \} \Rightarrow \{ chips \}$}}} - to every customer who buys beer and peanuts we should also recommend chips.
- {{{$ \{ tea \} \Rightarrow \{ sugar, lemon \} $}}} - to every customer who buys tea we also recommend sugar and lemon.

Every rule {{{$I \Rightarrow J$}}} has count and support in the databese of transactions. They are equal to the count and support of the itemset {{{$Q = I \cup J$}}} from which that rule was generated:

{{{
\[ \eta_T(I \Rightarrow J) = \eta_T(I \cup J) \]
\[ s_T(I \Rightarrow J) = s_T(I \cup J) \]
}}}

It is also a common practice to calculate ''confidence'' {{{$c_T(I \Rightarrow J)$}}}  and ''lift'' {{{$l_T(I \Rightarrow J)$}}} of an association rule and use them to filter rules by specifying minimum confidence {{{$c_{min}$}}} or minimum lift {{{$l_{min}$}}} threshold.

!!!!! Confidence

''Confidence'' of an association rule {{{$I \Rightarrow J$}}} is a conditional probability of itemset {{{$J$}}} appearing in an arbitrary transaction {{{$t \in T$}}} given that the itemset {{{$I$}}} has appeared in that transaction:

{{{
\[ c_T(I \Rightarrow J) = P(J|I) = \frac{\eta_T(I \cup J)}{\eta_T(I)} = \frac{s_T(I \cup J)}{s_T(I)} \]
}}}

In terms of basket anlysis, confidence {{{$c_T(I \Rightarrow J)$}}} tells us what percent of customers who purchased all products from itemset {{{$I$}}} have also purchased all products from itemset {{{$J$}}}.

!!!!! Lift

''Lift'' of an association rule {{{$I \Rightarrow J$}}} is a measure of correlation between {{{$J$}}} and {{{$I$}}}:

{{{
\[ l_T(I \Rightarrow J) = \frac{P(I \cup J)}{P(I)P(J)} = \frac{s_T(I \cup J)}{s_T(I)s_T(J)} \]
}}}

If occurences of {{{$I$}}} and {{{$J$}}} are independent, then {{{$P(I \cup J) = P(I)P(J)$}}} and {{{$l_T(I \Rightarrow J) = 1$}}}. Otherwise, if {{{$l_T(I \Rightarrow J) > 1$}}}, this indicates a positive correlation (people who purchased {{{$I$}}} are likely to also purchase {{{$J$}}}) and if {{{$l_T(I \Rightarrow J) < 1$}}}, then the correlation is negative (people who purchased {{{$I$}}} will most likely not purchase {{{$J$}}}).

Lift {{{$l_T(I \Rightarrow J)$}}} can also be interpreted as a measure of how much the relative frequency of {{{$J$}}} will increase if transactions are restricted to only those that contain {{{$I$}}}:

{{{
\[ l_T(I \Rightarrow J) = \frac{c_T(I \Rightarrow J)}{c_T(\varnothing \Rightarrow J)} = \frac{c_T(I \Rightarrow J)}{s_T(J)} \]
}}}

We invite you to prove that those two definitions of lift are equivalent.

!!!! Why this is a complicated problem?

It may seem that we can find all frequent itemsets in a database of transactions simply by iterating over all possible itemsets and selecting the ones that pass the minimum support threshold.

However, the number of possible itemsets grows very quickly as you increase the size of an item base {{{$B$}}}. Turns out that even for the smallest stores that sell very limited amount of products, a set of all possible combinations of those products will be so big that any analysis on this set becomes infeasible.

Let's take a closer look at this number. If {{{$B$}}} is the item base (for example, a list of all products sold by a supermarket), then the collection of all possible subsets of {{{$B$}}} is called ''powerset'' and denoted {{{$\mathbb{P}(B)$}}}.

{{{
\[ |\mathbb{P}(B)| = 2^{|B|} \]
}}}

+Hasse diagrams showing sets of all possible itemsets as powersets over the itembase B = {a}, B = {a, b}, and B = {a, b, c}. You can see how fast these diagrams are growing.>figures/Powersets.png+

!!!! The A-Priori Property

The idea behind A-Priori algorithm is based on a simple intuitive property of itemsets: ''if itemset I appears in transactions k times, then there can be no itemset J that contains all elements from I and appers more than k times''.

In other words,

{{{
\[ \forall I \subseteq J \subseteq B\ \colon\quad \eta_T(I) \geq \eta_T(J) \]
}}}

This fact will be central to everything what comes next, so we will give a small example to convince you in its validity.

You are given two itemsets

{{{
\begin{align*}
I &= \{ \text{bread}, \text{butter} \} \\
J &= \{ \text{bread}, \text{butter}, \text{salt} \}
\end{align*}
}}}

{{{$J$}}} is a superset of {{{$I$}}} because it contains all items of {{{$I$}}} (bread and butter) and some other items (salt).

Suppose that in a database of transactions {{{$T$}}} itemset {{{$I$}}} appears 3 times, meaning that there were 3 customers who bought bread and butter.

{{{
\[ \eta_T(I) = 3 \]
}}}

Without knowing anything else about the transaction dataset, we can be certain that itemset {{{$J$}}} could not appear in it more than 3 times. Why? Imagine that it's not the case, and the count of {{{$J$}}} is greater, let's say that it's equal to 5

{{{
\[ \eta_T(J) = 5 \]
}}}

This would mean that there were 5 customers, who bought bread, butter, and salt. But then, each one of them bought bread and butter, which means that {{{$\eta_T(I)$}}} is at least 5 (it can be greater because there may be customers who bought bread and butter but no salt).

Therefore, if an itemset is not frequent, then no superset of this itemset can be frequent:

{{{
\[ \forall I \subseteq J \subseteq B\ \colon\quad s_T(I) < s_{min}\ \ \Rightarrow\ \ s_T(J) < s_{min} \]
}}}

+Frequent Itemsets>figures/FrequentItemsets.png+

!!!! A-Priori Algorithm

We start the A-Priori algorithm by what is often called an ''"initialization step"'' - we build a set {{{$L_1$}}} of all frequent itemsets of size 1. We think of every item {{{$i$}}} from the item base {{{$B$}}} as an itemset with 1 element {{{$I = \{ i \}$}}}. Then we select only those itemsets whose support {{{$s_t(I$}}} is greater than or equal to the given minimum support {{{$s_{min}$}}}.

Then we repeat the following two steps with {{{$k = 2, 3, \dots$}}} as long as the set {{{$L_{k-1}$}}} of frequent {{{$(k-1)$}}}-itemsets is not empty:

# ''Candidate generation'': we use the set {{{$L_{k-1}$}}} of frequent {{{$(k-1)$}}}-itemsets to generate {{{$C_k$}}} - a set candidate {{{$k$}}}-itemsets.
# ''Frequent itemset selection'': we construct the set {{{$L_k$}}} of frequent {{{$k$}}}-itemsets by selecting only those candidates from {{{$C_k$}}} that pass the minimem support threshold.

To get a better intuition on the flow of the A-Priori algorithm, take a look on Figure *@figAPriori*. We start with a set of frequent items {{{$L_1$}}}, use it to generate candidates {{{$C_2$}}}, then select frequent pairs {{{$L_2$}}}, and continue this process until we get an empty set {{{$L_k = \varnothing$}}}.

+The flow of A-Priori algorithm>figures/APriori.png|label=figAPriori+

The whole algorithm is formally described with pseudocode as Function *@funAPriori*. The function ==generate_candidates== will be defined in the next section, where we will talk about the process of candidate generation.

{{{
\begin{algorithm}[H]
 \KwData{Database of transactions $T$ and a support threshold $s_{min}$}
 \KwResult{A set $F_T(s_{min})$ of all frequent itemsets in $T$}
 \Fn{apriori(T, $s_{min}$)}{
 	$L_1 \gets$ \{frequent 1-itemsets\}\\
 	\For{($k = 2;\ L_{k-1} \neq \varnothing;\ k++$)}{
 		$C_k \gets$ generate\_candidates($L_{k-1}$)\\
		$L_k \gets \{ I \in C_k | s_T(I) \geq s_{min} \}$
 	}
 	$F_T(s_{min}) \gets \bigcup_k L_k$ \\
	\KwRet $F_T(s_{min})$
 }
 \caption{A-Priori algorithm}
 \label{funAPriori}
\end{algorithm}
}}}

!!!!! Candidate Generation for the A-Priori Algorithm
@sec:APriori-CandidateGeneration

In this section, we will take a closer look at the candidate generation for the A-Priori algorithm. At every iteration, it uses previously generated set {{{$L_{k-1}$}}} of frequent {{{$(k-1)$}}}-itemsets to generate a set {{{$C_k$}}} containing candidate itemsets of size {{{$k$}}}. This is done in two steps:

% I write it in LaTeX because I need to insert equations into the list environment without breaking it into multiple lists
{{{
\begin{enumerate}
\item \textbf{Join step} - we join the set $L_{k-1}$ with itself (this operation is denoted as $L_{k-1} \bowtie L_{k-1}$) according to the rules
\begin{itemize}
\item Every two itemsets $I = \{ i_1, i_2, \dots, i_{k-1} \}$ and $J = \{ j_1, j_2, \dots, j_{k-1} \}$ selected from $L_{k-1}$ can be joined if their first $k-2$ elements are the same and the last element of $I$ is smaller than the last element of $J$ (this ensures that the resulted itemset will be sorted and there will be no duplicate candidates)

\[ (i_1 = j_1) \land (i_2 = j_2) \land \dots \land (i_{k-2} = j_{k-2}) \land (i_{k-1} < j_{k-1}) \]

\item If two itemsets $I$ and $J$ can be joined, we join them into a $k$-itemset $Q$ by taking the first $k-2$ elements that they have in common, and appending to them the last element of $I$ followed by the last element of $J$

\[ Q = \{ i_1, i_2, \dots, i_{k-2}, i_{k-1}, j_{k-1} \} \]
\end{itemize}

\item \textbf{Prune step} - we remove all candidates that have at least one subset of size $k-1$ that is not in $L_{k-1}$.
\end{enumerate}
}}}

You can see the pseudocode of the candidate generation procedure in Function *@funCandidateGeneration*.

{{{
\begin{algorithm}
 \KwData{A set of frequent ($k-1$)-itemsets $L_{k-1}$}
 \KwResult{A set of candidate $k$-itemsets $C_k$}
 \Fn{generate\_candidates($L_{k-1}$)}{
 	$C_k \gets$ \{\}\\
 	\ForAll(\tcp*[f]{join step}){$I \in L_{k-1}$}{
 		\ForAll{$J \in L_{k-1}$}{
			\If{can\_be\_joined($I$, $J$)}{
 				$C_k$.add(join($I$, $J$))
			}
		}
 	}
 	\ForAll(\tcp*[f]{prune step}){$I \in C_k$}{
 		\ForAll{($k-1$)-subsets $I_{k-1} \subset I$} {
			\If{$I_{k-1} \notin L_{k-1}$}{
				$C_k$.remove($I$)\\
				break
			}
		}
 	}
	\KwRet $C_k$
 }
 \Fn{can\_be\_joined(I, J)}{
 	\For{$p \gets 1$ \KwTo $k-2$}{
 		\If{$i_p$ != $j_p$}{
			\KwRet false
		}
	 }
	 \KwRet $i_{k-1} < j_{k-1}$
 }
 \Fn{join(I, J)}{
 	$Q \gets \{\}$ \\
 	\For{$p \gets 1$ \KwTo $k-1$}{
 		Q.add($j_p$)
 	}
 	Q.add($j_{k-1}$) \\
 	\KwRet Q
 }
 \caption{Candidate generation for the A-Priori algorithm}
 \label{funCandidateGeneration}
\end{algorithm}
}}}

!!!! Proving the correctness of candidate generation

It may not be obvious that by joining the set {{{$L_{k-1}$}}} of frequent {{{$(k-1)$}}}-itemsets with itself, using the join operation {{{$L_{k-1} \bowtie L_{k-1}$}}} defined above, we cover all possible itemsets of size {{{$k$}}}. In other words, we need to show that {{{$L_k \subseteq C_k$}}}.

Let's show that an arbitrary frequent {{{$k$}}}-itemset {{{$I_k \in L_k$}}} will be included into {{{$C_k$}}} when we generate {{{$C_k$}}} from {{{$L_{k-1}$}}}:

{{{
\[ I_k = \{ i_1, i_2, \dots, i_k \} \]
}}}

We can extract the following two subsets of size {{{$k-1$}}} from itemset {{{$I_k$}}}:

{{{
\begin{align*}
I_{k-1}^{(1)} &= \{ i_1, i_2, \dots, i_{k-2}, i_{k-1} \} \\
I_{k-1}^{(2)} &= \{ i_1, i_2, \dots, i_{k-2}, i_k \}
\end{align*}
}}}

Based on the A-Priori property, which tells us that every subset of a frequent itemset is also frequent, both {{{$I_{k-1}^{(1)}$}}} and {{{$I_{k-1}^{(2)}$}}} belong to {{{$L_{k-1}$}}} - the set of frequent itemsets of size {{{$k-1$}}}.

{{{
\[ I_{k-1}^{(1)}, I_{k-1}^{(2)} \in L_{k-1} \]
}}}

You might have noticed that we selected {{{$I_{k-1}^{(1)}$}}} and {{{$I_{k-1}^{(2)}$}}} in such way that they satisfy the A-Priori's join condition described in Function *@funJoinCondition*:	

# They share the first {{{$k-2$}}} items
# The last element of {{{$I_{k-1}^{(1)}$}}} is smaller than the last element of {{{$I_{k-1}^{(2)}$}}} because elements of itemset {{{$I_k$}}} are sorted and therefore {{{$i_{k-1} < i_k$}}}

This means that during the join step on set {{{$L_{k-1}$}}}, itemsets {{{$I_{k-1}^{(1)}$}}} and {{{$I_{k-1}^{(2)}$}}} will be joined and therefore:

{{{
\[ I_k \in L_{k-1} \bowtie L_{k-1} \]
}}}

{{{$I_k$}}} will not be removed during the prune step because, as we said before, based on the A-Priori property, all subsets of {{{$I_k$}}} are frequent.

Therefore,

{{{
\[ \forall I_k \in L_k \quad I_k \in C_k \]
}}}

Which means that

{{{
\[ L_k \subseteq C_k \]
}}}

!!! Simple Example
@sec:APriori-SimpleExample

In this section, we will give a simple example of finding frequent itemsets and association rules using A-Priori algorithm.

Given a support threshold {{{$s_{min} = 1/3$}}}, we need to find frequent itemsets in the following database of transactions {{{$T$}}}:

|! TID |! Transaction
| 1 | {eggs, milk, butter}
| 2 | {milk, cereal}
| 3 | {eggs, bacon}
| 4 | {bread, butter}
| 5 | {bread, bacon, eggs}
| 6 | {bread, avocado, butter, bananas}

This means that we have to identify all possible combinations of products that appear in at least 33.33\% of transactions (at least 2 out of 6).

We will then use those frequent itemsets to produce association rules for the two values of minimum confidence threshold: {{{$c_{min} = 2/3$}}} and {{{$c_{min} = 1$}}}.

!!!! Mining Frequent Itemsets

The item base is the set of all products that appear in transactions:

{{{
\[ B = \{ eggs, milk, butter, cereal, bacon, bread, avocado, bananas \} \]
}}}

We start by encoding every product in the item base with an integer number:

|! Product |! Code |! Product |!Code
| eggs | 1 | bacon | 5
| milk | 2 | bread | 6
| butter | 3 | avocado | 7
| cereal | 4 | bananas | 8

This gives us the encoded database of transactions:

|! TID |! Transaction
| 1 | {1, 2, 3}
| 2 | {2, 4}
| 3 | {1, 5}
| 4 | {3, 6}
| 5 | {1, 5, 6}
| 6 | {3, 6, 7, 8}

To initialize the A-Priori algorithm, we need to construct the set {{{$L_1$}}} containing all frequent 1-itemsets. To do that, we first create a set of candidates {{{$C_1$}}} - all items from the encoded item base {{{$B$}}} represented as itemsets of size 1:

{{{
\[ C_1 = \{ \{1\}, \{2\}, \{3\}, \{4\}, \{5\}, \{6\}, \{7\}, \{8\} \} \]
}}}

Now we calculate support of each candidate itemset:

{{{
\begin{center}
\begin{tabular}{ll}
$s_T(\{1\}) = 1/2$ & $s_T(\{5\}) = 1/3$ \\
$s_T(\{2\}) = 1/3$ & $s_T(\{6\}) = 1/2$ \\
$s_T(\{3\}) = 1/2$ & $s_T(\{7\}) = 1/6$ \\
$s_T(\{4\}) = 1/6$ & $s_T(\{8\}) = 1/6$ \\
\end{tabular}
\end{center}
}}}

We can see that itemsets {{{$\{4\}$}}}, {{{$\{7\}$}}}, and {{{$\{8\}$}}} (cereal, avocado, and bananas) did not pass the minimum support threshold of 1/3. They don't appear in transactions often enough to be of any interest to us. And based on the A-Priori property, any set of items that includes at least one of those items, can not have higher support than 1/6. Therefore, items 4, 7, and 8 and all their supersets will be excluded from further analysis.

Other itemsets have passed the support threshold, which means that they are frequent. We collect them into the set of frequent 1-itemsets:

{{{
\[ L_1 = \{ \{1\}, \{2\}, \{3\}, \{5\}, \{6\} \} \]
}}}

Now we generate the candidates of size 2. To do that, we first join the set of frequent 1-itemsets {{{$L_1$}}} with itself according to the rules of a join step described in Section *@sec:APriori-CandidateGeneration*:

{{{
\[ L_1 \bowtie L_1 = \{ \{1,2\}, \{1, 3\}, \{1, 5\}, \{1, 6\}, \{2, 3\}, \{2, 5\}, \{2, 6\}, \{3, 5\}, \{3, 6\}, \{5, 6\} \} \]
}}}

Prune step of the algorithm deletes all itemsets of size {{{$k$}}} that contain at least one subset of size {{{$k-1$}}} which is not in {{{$L_{k-1}$}}}. For {{{$k=2$}}}, all itemsets included in {{{$L_1 \bowtie L_1$}}} are composed only of 1-itemsets taken from {{{$L_1$}}}. So the prune step will not delete any items and the set of candidate itemsets is the same as the result of a join step:

{{{
\[ C_2 = L_1 \bowtie L_1 \]
}}}

Once again, we calculate support of all candidate itemsets:

{{{
\begin{center}
\begin{tabular}{ll}
$s_T(\{1,2\}) = 1/6$ & $s_T(\{2,5\}) = 0$ \\
$s_T(\{1,3\}) = 1/6$ & $s_T(\{2,6\}) = 0$ \\
$s_T(\{1,5\}) = 1/3$ & $s_T(\{3,5\}) = 0$ \\
$s_T(\{1,6\}) = 1/6$ & $s_T(\{3,6\}) = 1/3$ \\
$s_T(\{2,3\}) = 1/6$ & $s_T(\{5,6\}) = 1/6$ \\
\end{tabular}
\end{center}
}}}

Only two itemsets have passed the support thresholds, so only they are selected into the set of all frequent 2-itemsets:

{{{
\[ L_2 = \{ \{1,5\}, \{3,6\} \} \]
}}}

On the next iteration, join step produces an empty set, because the only pair of itemsets from {{{$L_2$}}} does not satisfy the join conditions listed in Section *@sec:APriori-CandidateGeneration* and can not be joined:

{{{
\[ L_2 \bowtie L_2 = \varnothing \]
}}}

This means that the set of candidates {{{$C_3$}}} and the set of frequent itemsets {{{$L_3$}}} are also empty and the algorithm stops here:

{{{
\[ C_3 = L_3 = \varnothing \]
}}}

The set of all frequent itemsets that we were trying to find is the union of {{{$L_1$}}} and {{{$L_2$}}}:

{{{
\[ F_T(s_{min} = 1/3) = L_1 \cup L_2 = \{ \{1\}, \{2\}, \{3\}, \{5\}, \{6\}, \{1,5\}, \{3,6\} \} \]
}}}

We can now decode them using our encoding table. Below, you can find the list of decoded frequent itemsets together with their count and support values (remember that count is support multiplied by the total number of transactions {{{$|T| = 6$}}}). These are all possible sets of products that appear in at least 33.33\% of transactions:

|! Frequent itemset |! Count |! Support
| {eggs} | 3 | 1/2
| {milk} | 2 | 1/3
| {butter} | 3 | 1/2
| {bacon} | 2 | 1/3
| {bread} | 3 | 1/2
| {eggs, bacon} | 2 | 1/3
| {butter, bread} | 2 | 1/3

!!!! Producing Association Rules

We can represent any itemset with at least 2 elements as an association rule. In our case, we only have two itemsets with more than 1 element: {1, 5} and {3, 6}.

The first itemset {1, 5} (eggs and bacon) can be represented as an association rule in two different ways:

{{{
\[ \{ 1 \} \Rightarrow \{ 5 \} \]
\[ \{ 5 \} \Rightarrow \{ 1 \} \]
}}}

First rule means that to every customer who buys item 1 (eggs), we recommend item 5 (bacon). Second rule goes the other way around - to every customer who buys bacon, we recommend eggs. Notice that these are different rules and one of them may be more relevant than the other.

To measure the relevance of these rules, we calculate their confidence:

{{{
\[ c_T(\{ 1 \} \Rightarrow \{ 5 \}) = \frac{s_T(\{ 1, 5 \})}{s_T(\{ 1 \})} = \frac23 \]
\[ c_T(\{ 5 \} \Rightarrow \{ 1 \}) = \frac{s_T(\{ 1, 5 \})}{s_T(\{ 5 \})} = 1 \]
}}}

We can see that 2 out of 3 customers who bought eggs have also bought bacon. And 100\% of customers who bought bacon have also bought eggs.

We can use the value of confidence to calculate the lift of those rules:

{{{
\[ l_T(\{ 1 \} \Rightarrow \{ 5 \}) = \frac{c_T(\{ 1 \} \Rightarrow \{ 5 \})}{s_T(\{ 5 \})} = 2 \]
\[ l_T(\{ 5 \} \Rightarrow \{ 1 \}) = \frac{c_T(\{ 5 \} \Rightarrow \{ 1 \})}{s_T(\{ 1 \})} = 2 \]
}}}

Now let's produce rules from the second itemset {3, 6} (butter and bread):

{{{
\[ \{ 3 \} \Rightarrow \{ 6 \} \]
\[ \{ 6 \} \Rightarrow \{ 3 \} \]
}}}

Again, we calculate the confidence of both rules:

{{{
\[ c_T(\{ 3 \} \Rightarrow \{ 6 \}) = \frac{s_T(\{ 3, 6 \})}{s_T(\{ 3 \})} = \frac23 \]
\[ c_T(\{ 6 \} \Rightarrow \{ 3 \}) = \frac{s_T(\{ 3, 6 \})}{s_T(\{ 6 \})} = \frac23 \]
}}}

And their lift:

{{{
\[ l_T(\{ 3 \} \Rightarrow \{ 6 \}) = \frac{c_T(\{ 3 \} \Rightarrow \{ 6 \})}{s_T(\{ 6 \})} = 4/3 \]
\[ l_T(\{ 6 \} \Rightarrow \{ 3 \}) = \frac{c_T(\{ 6 \} \Rightarrow \{ 3 \})}{s_T(\{ 3 \})} = 4/3 \]
}}}

Now we decode the items and present rules together with their count, support, confidence, and lift. Count and support of each rule is the same as count and support of the frequent itemset from which this rule was extracted.

|! Association rule |! Count |! Support |! Confidence |! Lift
| {{{$ \{ eggs \} \Rightarrow \{ bacon \} $}}} | 2 | 1/3 | 2/3 | 2
| {{{$ \{ bacon \} \Rightarrow \{ eggs \} $}}} | 2 | 1/3 | 1 | 2
| {{{$ \{ butter \} \Rightarrow \{ bread \} $}}} | 2 | 1/3 | 2/3 | 4/3
| {{{$ \{ bread \} \Rightarrow \{ butter \} $}}} | 2 | 1/3 | 2/3 | 4/3

With the minimum confidence threshold {{{$c_{min} = 2/3$}}}, all four rules will be selected. And if we set the confidence threshold {{{$c_{min} = 1$}}}, only the rule {{{$ \{ bacon \} \Rightarrow \{ eggs \} $}}} will pass it.

Here is a way to interpret this: According to our database of transactions, 100\% of customers who bought bacon have also bought eggs. This statement is supported by 33.33\% of transactions.

!!! Designing the API
@sec:APriori-API

[[[
groceries := #( 
  (eggs milk butter)
  (milk cereal)
  (eggs bacon)
  (bread butter)
  (bread bacon eggs)
  (bread avocado butter bananas)).
]]]

[[[
apriori := APriori
  transactions: transactions
  supportThreshold: 1/3
  confidenceThreshold: 1/3.
]]]

[[[
itemsets := apriori frequentItemsets.
rules := apriori associationRules.
]]]

[[[
itemsets first count.
itemsets first support.
]]]

[[[
rules first count.
rules first support.
rules first confidence.
rules first lift.
]]]

!!! Writing tests
@sec:APriori-Tests


!!! Implementation
@sec:APriori-Implementation


!!! Practical Examples
@sec:APriori-PracticalExamples

!!! Recommended Reading
@sec:APriori-RecommendedReading

# ""Fast Algorithm for Mining Association Rules"" by Rakesh Agrawal and Ramakrishnan Srikant ${cite:Agra94a}$
# ""Frequent item set mining"" by Christian Borgelt ${cite:Borg12a}$
# Chapter 6 of ""Data Mining: Concepts and Techniques"" by Jiawei Han, Micheline Kamber, and Jian Pei ${cite:Han11a}$
# Chapter 6 of ""Mining of Massive Datasets"" by Jure Leskovec, Anand Rajaraman, and Jeffrey D. Ullman ${cite:Lesk14a}$

!!! List of notations used in this chapter
@sec:APriori-Notation

|! Symbol |! Interpretation
| {{{$i_1, \dots, i_k$}}} | items (products)
| {{{$t_1, \dots, t_m$}}} | transactions
| {{{$I, J, Q$}}} | itemsets
| {{{$I_k, J_k, Q_k$}}} | itemsets of size k or k-itemsets
| {{{$I \Rightarrow J$}}} | association rule
| {{{$B$}}} | item base
| {{{$T$}}} | database of transactions
| {{{$K_T(I)$}}} | cover of itemset {{{$I$}}} in a database of transactions {{{$T$}}}
| {{{$\eta_T(I)$}}} | count of itemset {{{$I$}}} in a database of transactions {{{$T$}}}
| {{{$s_T(I)$}}} | support of itemset {{{$I$}}} in a database of transactions {{{$T$}}}
| {{{$c_T(I \Rightarrow J)$}}} | confidence of association rule {{{$I \Rightarrow J$}}} in a database of transactions {{{$T$}}}
| {{{$l_T(I \Rightarrow J)$}}} | lift of association rule {{{$I \Rightarrow J$}}} in a database of transactions {{{$T$}}}
| {{{$s_{min}$}}} | minimum support threshold
| {{{$c_{min}$}}} | minimum confidence threshold
| {{{$l_{min}$}}} | minimum lift threshold
| {{{$C_k$}}} | set of candidate itemsets of size k
| {{{$L_k$}}} | set of frequent itemsets of size k
| {{{$L_k \bowtie L_k$}}} | set {{{$L_k$}}} joined with itself according to the rules defined in this chapter
| {{{$F_T(s_{min})$}}} | set of all frequent itemsets in the database of transactions {{{$T$}}} with minimum support {{{$s_{min}$}}}
| {{{$\mathbb{P}(B)$}}} | powerset of {{{B}}}
| {{{$P(J \vert I)$}}} | conditional probability of {{{$J$}}} given {{{$I$}}}
